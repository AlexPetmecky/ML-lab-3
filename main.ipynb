{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#Task and Use-Case \n",
    "We are trying to predict the amount of alchohol people drink during the workweek based on their sex, age, mother's education, fathers education, Mother's job, father's job, main guardian, class failures, romantic status, and family relations. \n",
    "\n",
    "A classifier like this could be incredibly helpful because it could help get people the mental health services they need before turning to alcohol to drown out their problems, especially for students, which is who this data was collected from\n",
    "\n",
    "This model could be deployed in Universities across the world to help health centers reach out to potential at-risk individuals from falling into an addiction\n",
    "\n",
    "For this task a classifier with a prediction rate of "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "184f232537d73e20"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-11T21:06:40.493502Z",
     "start_time": "2023-10-11T21:06:40.468425Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sex  age  Medu  Fedu      Mjob      Fjob guardian  failures romantic  \\\n",
      "0      F   18     4     4   at_home   teacher   mother         0       no   \n",
      "1      F   17     1     1   at_home     other   father         0       no   \n",
      "2      F   15     1     1   at_home     other   mother         3       no   \n",
      "3      F   15     4     2    health  services   mother         0      yes   \n",
      "4      F   16     3     3     other     other   father         0       no   \n",
      "...   ..  ...   ...   ...       ...       ...      ...       ...      ...   \n",
      "1039   F   19     2     3  services     other   mother         1       no   \n",
      "1040   F   18     3     1   teacher  services   mother         0       no   \n",
      "1041   F   18     1     1     other     other   mother         0       no   \n",
      "1042   M   17     3     1  services  services   mother         0       no   \n",
      "1043   M   18     3     2  services     other   mother         0       no   \n",
      "\n",
      "      famrel  \n",
      "0          4  \n",
      "1          5  \n",
      "2          4  \n",
      "3          3  \n",
      "4          4  \n",
      "...      ...  \n",
      "1039       5  \n",
      "1040       4  \n",
      "1041       1  \n",
      "1042       2  \n",
      "1043       4  \n",
      "\n",
      "[1044 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#X = np.load(\"Telecust1.csv\")\n",
    "#X= np.genfromtxt('Telecust1.csv',delimiter=',',dtype=None)\n",
    "#print(X)\n",
    "df = pd.read_csv('data.csv')\n",
    "#print(df[[\"sex\",\"age\",\"Medu\",\"Fedu\",\"Mjob\",\"Fjob\",\"guardian\",\"failures\",\"romantic\",\"famrel\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Data Information"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a5e0d7a9261bbf86"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               age         Medu         Fedu     failures       famrel\n",
      "count  1044.000000  1044.000000  1044.000000  1044.000000  1044.000000\n",
      "mean     16.726054     2.603448     2.387931     0.264368     3.935824\n",
      "std       1.239975     1.124907     1.099938     0.656142     0.933401\n",
      "min      15.000000     0.000000     0.000000     0.000000     1.000000\n",
      "25%      16.000000     2.000000     1.000000     0.000000     4.000000\n",
      "50%      17.000000     3.000000     2.000000     0.000000     4.000000\n",
      "75%      18.000000     4.000000     3.000000     0.000000     5.000000\n",
      "max      22.000000     4.000000     4.000000     3.000000     5.000000\n"
     ]
    }
   ],
   "source": [
    "print(df[[\"sex\",\"age\",\"Medu\",\"Fedu\",\"Mjob\",\"Fjob\",\"guardian\",\"failures\",\"romantic\",\"famrel\"]].describe())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T06:04:50.246678Z",
     "start_time": "2023-10-12T06:04:50.197252Z"
    }
   },
   "id": "2a031a6af40bcd83"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "#the base Binary Logistic Regression class\n",
    "class BinaryLogisticRegressionBase:\n",
    "    # private:\n",
    "    def __init__(self, eta, iterations=20):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'Base Binary Logistic Regression Object, Not Trainable'\n",
    "    \n",
    "    # convenience, private and static:\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        return 1/(1+np.exp(-theta)) \n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_intercept(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    # public:\n",
    "    def predict_proba(self, X, add_intercept=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_intercept(X) if add_intercept else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T21:06:40.501905Z",
     "start_time": "2023-10-11T21:06:40.500751Z"
    }
   },
   "id": "bba2e84bf2ca35f3"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "#Binary Logistic regression, inheriting from the base\n",
    "class BinaryLogisticRegression(BinaryLogisticRegressionBase):\n",
    "    #private:\n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    def _get_gradient(self,X,y):\n",
    "        # programming \\sum_i (yi-g(xi))xi\n",
    "        gradient = np.zeros(self.w_.shape) # set gradient to zero\n",
    "        for (xi,yi) in zip(X,y):\n",
    "            # the actual update inside of sum\n",
    "            gradi = (yi - self.predict_proba(xi,add_intercept=False))*xi \n",
    "            # reshape to be column vector and add to gradient\n",
    "            gradient += gradi.reshape(self.w_.shape) \n",
    "        \n",
    "        return gradient/float(len(y))\n",
    "       \n",
    "    # public:\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_intercept(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T21:06:40.509421Z",
     "start_time": "2023-10-11T21:06:40.508078Z"
    }
   },
   "id": "8684696c9f200882"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "#Vector binary logistic regression\n",
    "class VectorBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    # inherit from our previous class to get same functionality\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    # but overwrite the gradient calculation\n",
    "    def _get_gradient(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_intercept=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        return gradient.reshape(self.w_.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T21:06:41.220555Z",
     "start_time": "2023-10-11T21:06:40.513611Z"
    }
   },
   "id": "6a3d8b3f9675e924"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creating 1 vs All logistic Regression"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26f0af809bf94cf5"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained MultiClass Logistic Regression Object\n"
     ]
    }
   ],
   "source": [
    "#ONE VS ALL LOGISTIC REGRESSION\n",
    "#this takes each class and creates a binary classifier for it, and then runs each instance through all binary classifiers and choose the highest one\n",
    "class LogisticRegression:\n",
    "    def __init__(self, eta, iterations=20):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.unique(y) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = [] # will fill this array with binary classifiers\n",
    "        \n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = (y==yval) # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            blr = VectorBinaryLogisticRegression(self.eta,\n",
    "                                                 self.iters)\n",
    "            blr.fit(X,y_binary)\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(blr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for blr in self.classifiers_:\n",
    "            probs.append(blr.predict_proba(X)) # get probability for each classifier\n",
    "        \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return self.unique_[np.argmax(self.predict_proba(X),axis=1)] # take argmax along row\n",
    "    \n",
    "lr = LogisticRegression(0.1,1500)\n",
    "print(lr)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T21:06:41.226194Z",
     "start_time": "2023-10-11T21:06:41.224569Z"
    }
   },
   "id": "c5fd074bd10dda29"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using 1 vs All Logistic Regression"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b7d3c2468d9198"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sex  age  Medu  Fedu      Mjob      Fjob guardian  failures romantic  \\\n",
      "145    F   15     1     1     other  services   father         0       no   \n",
      "590    M   16     3     3  services     other   mother         0      yes   \n",
      "482    F   15     4     2     other     other   mother         0       no   \n",
      "251    M   16     3     3   at_home     other    other         0       no   \n",
      "630    F   17     1     1   at_home     other   mother         0      yes   \n",
      "...   ..  ...   ...   ...       ...       ...      ...       ...      ...   \n",
      "1033   M   17     2     3     other  services   father         0       no   \n",
      "763    M   18     4     4     other     other   father         0       no   \n",
      "835    M   16     1     1   at_home  services   mother         0      yes   \n",
      "559    M   16     2     1   at_home     other   mother         1      yes   \n",
      "684    F   17     4     2   teacher  services   mother         0       no   \n",
      "\n",
      "      famrel  \n",
      "145        4  \n",
      "590        4  \n",
      "482        5  \n",
      "251        5  \n",
      "630        4  \n",
      "...      ...  \n",
      "1033       4  \n",
      "763        4  \n",
      "835        5  \n",
      "559        4  \n",
      "684        4  \n",
      "\n",
      "[835 rows x 10 columns]\n",
      "145     1\n",
      "590     1\n",
      "482     1\n",
      "251     1\n",
      "630     1\n",
      "       ..\n",
      "1033    1\n",
      "763     3\n",
      "835     4\n",
      "559     3\n",
      "684     1\n",
      "Name: Dalc, Length: 835, dtype: int64\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't multiply sequence by non-int of type 'float'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[23], line 16\u001B[0m\n\u001B[1;32m     14\u001B[0m logReg \u001B[38;5;241m=\u001B[39m LogisticRegression(\u001B[38;5;241m0.1\u001B[39m,\u001B[38;5;241m500\u001B[39m) \u001B[38;5;66;03m#eta and iterations\u001B[39;00m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m#print(logReg)\u001B[39;00m\n\u001B[0;32m---> 16\u001B[0m \u001B[43mlogReg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m#print(logReg)\u001B[39;00m\n\u001B[1;32m     18\u001B[0m yhat \u001B[38;5;241m=\u001B[39m lr\u001B[38;5;241m.\u001B[39mpredict(X_test)\n",
      "Cell \u001B[0;32mIn[16], line 25\u001B[0m, in \u001B[0;36mLogisticRegression.fit\u001B[0;34m(self, X, y)\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;66;03m# train the binary classifier for this class\u001B[39;00m\n\u001B[1;32m     23\u001B[0m blr \u001B[38;5;241m=\u001B[39m VectorBinaryLogisticRegression(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39meta,\n\u001B[1;32m     24\u001B[0m                                      \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miters)\n\u001B[0;32m---> 25\u001B[0m \u001B[43mblr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43my_binary\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;66;03m# add the trained classifier to the list\u001B[39;00m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclassifiers_\u001B[38;5;241m.\u001B[39mappend(blr)\n",
      "Cell \u001B[0;32mIn[14], line 30\u001B[0m, in \u001B[0;36mBinaryLogisticRegression.fit\u001B[0;34m(self, X, y)\u001B[0m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;66;03m# for as many as the max iterations\u001B[39;00m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miters):\n\u001B[0;32m---> 30\u001B[0m     gradient \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_gradient\u001B[49m\u001B[43m(\u001B[49m\u001B[43mXb\u001B[49m\u001B[43m,\u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     31\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mw_ \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m gradient\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39meta\n",
      "Cell \u001B[0;32mIn[15], line 12\u001B[0m, in \u001B[0;36mVectorBinaryLogisticRegression._get_gradient\u001B[0;34m(self, X, y)\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_get_gradient\u001B[39m(\u001B[38;5;28mself\u001B[39m,X,y):\n\u001B[0;32m---> 12\u001B[0m     ydiff \u001B[38;5;241m=\u001B[39m y\u001B[38;5;241m-\u001B[39m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict_proba\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43madd_intercept\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mravel() \u001B[38;5;66;03m# get y difference\u001B[39;00m\n\u001B[1;32m     13\u001B[0m     gradient \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmean(X \u001B[38;5;241m*\u001B[39m ydiff[:,np\u001B[38;5;241m.\u001B[39mnewaxis], axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m) \u001B[38;5;66;03m# make ydiff a column vector and multiply through\u001B[39;00m\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m gradient\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mw_\u001B[38;5;241m.\u001B[39mshape)\n",
      "Cell \u001B[0;32mIn[13], line 25\u001B[0m, in \u001B[0;36mBinaryLogisticRegressionBase.predict_proba\u001B[0;34m(self, X, add_intercept)\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict_proba\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, add_intercept\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[1;32m     23\u001B[0m     \u001B[38;5;66;03m# add bias term if requested\u001B[39;00m\n\u001B[1;32m     24\u001B[0m     Xb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_add_intercept(X) \u001B[38;5;28;01mif\u001B[39;00m add_intercept \u001B[38;5;28;01melse\u001B[39;00m X\n\u001B[0;32m---> 25\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sigmoid(\u001B[43mXb\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m@\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mw_\u001B[49m)\n",
      "\u001B[0;31mTypeError\u001B[0m: can't multiply sequence by non-int of type 'float'"
     ]
    }
   ],
   "source": [
    "#importing these specific split and metric functions\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "#using the test/train split function\n",
    "X = df[[\"sex\",\"age\",\"Medu\",\"Fedu\",\"Mjob\",\"Fjob\",\"guardian\",\"failures\",\"romantic\",\"famrel\"]] # modify \n",
    "y = df[\"Dalc\"] #pick a category that can be output\n",
    "#print(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,train_size=0.8, test_size=0.2, random_state=0)\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "logReg = LogisticRegression(0.1,500) #eta and iterations\n",
    "#print(logReg)\n",
    "logReg.fit(X_train,y_train)\n",
    "#print(logReg)\n",
    "yhat = lr.predict(X_test)\n",
    "print(\"Accuracy of: \",accuracy_score(y_test,yhat))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T21:35:49.237198Z",
     "start_time": "2023-10-11T21:35:49.132007Z"
    }
   },
   "id": "ae05a0b553f28d14"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Stochastic Logistic Regression"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c751a264163b248"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#i think it will work if we inherit logistic regression instead of binary logistic regression\n",
    "class StochasticLogisticRegression(LogisticRegression):\n",
    "    # stochastic gradient calculation \n",
    "    def _get_gradient(self,X,y):\n",
    "        \n",
    "        # grab a subset of samples in a mini-batch\n",
    "        # and calculate the gradient according to the small batch only\n",
    "        mini_batch_size = 16\n",
    "        idxs = np.random.choice(len(y), mini_batch_size)\n",
    "        \n",
    "        ydiff = y[idxs]-self.predict_proba(X[idxs],add_bias=False).ravel() # get y difference (now scalar)\n",
    "        gradient = np.mean(X[idxs] * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return gradient\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dae7cec38bec103"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using Stochastic Logistic Regression"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bcb16eded9e774c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "slr = StochasticLogisticRegression(eta=0.01, iterations=300, C=0.001)\n",
    "slr.fit(X_train,y_train)\n",
    "\n",
    "yhat = slr.predict(X_test)\n",
    "print(slr)\n",
    "print('Accuracy of: ',accuracy_score(y_test,yhat))  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2a643bbb02c770b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Setting up a vectorized version of Newton's update method, it runs significantly faster "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "51808d5bd639b6b1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from numpy.linalg import pinv\n",
    "#made it Logistic Regression instead of Binary Logistic Regression\n",
    "class HessianLogisticRegression(LogisticRegression):\n",
    "    # just overwrite gradient function\n",
    "    def _get_gradient(self,X,y):\n",
    "        g = self.predict_proba(X,add_bias=False).ravel() # get sigmoid value for all classes\n",
    "        hessian = X.T @ np.diag(g*(1-g)) @ X - 2 * self.C # calculate the hessian\n",
    "\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.sum(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return pinv(hessian) @ gradient\n",
    "       \n",
    "       \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c02a171d7dae064a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Using Hessian Logistic Regression (Vectorized Newton)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e1d9cf78b56d53e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hlr = HessianLogisticRegression(eta=1.0,\n",
    "                                      iterations=4,\n",
    "                                      C=0.001) # note that we need only a few iterations here\n",
    "\n",
    "hlr.fit(X_train,y_train)\n",
    "yhat = hlr.predict(X_test)\n",
    "print(hlr)\n",
    "print('Accuracy of: ',accuracy_score(y_test,yhat))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a96daa8a03d8fb1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Using Gradient Decent"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eede3caa606aff33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2b7402e30fc0bcc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#running code and choosing an optimazation type"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec151524cc6820f6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##this may need to go into the logistic regression class as different get gradient functions\n",
    "##accept a parameter that decides which gradient function to use"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5539cac52673500c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
